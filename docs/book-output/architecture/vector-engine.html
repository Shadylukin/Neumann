<!DOCTYPE HTML>
<html lang="en" class="rust sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Vector Engine - Neumann</title>


        <!-- Custom HTML head -->

        <meta name="description" content="Unified tensor-based runtime for relational, graph, and vector data">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "rust";
            const default_dark_theme = "coal";
            window.path_to_searchindex_js = "../searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('rust')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Neumann</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/Shadylukin/Neumann" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        <a href="https://github.com/Shadylukin/Neumann/edit/main/docs/book/src/architecture/vector-engine.md" title="Suggest an edit" aria-label="Suggest an edit" rel="edit">
                            <i id="git-edit-button" class="fa fa-edit"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="vector-engine"><a class="header" href="#vector-engine">Vector Engine</a></h1>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>Module 4 of Neumann. Provides embeddings storage and similarity search with
SIMD-accelerated distance computations.</p>
<p>The Vector Engine builds on <code>tensor_store</code> to provide k-NN search capabilities.
It supports both brute-force O(n) search and HNSW O(log n) approximate search,
with automatic sparse vector optimization for memory efficiency.</p>
<h2 id="design-principles"><a class="header" href="#design-principles">Design Principles</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Principle</th><th>Description</th></tr></thead><tbody>
<tr><td>Layered Architecture</td><td>Depends only on Tensor Store for persistence</td></tr>
<tr><td>Multiple Distance Metrics</td><td>Cosine, Euclidean, and Dot Product similarity</td></tr>
<tr><td>SIMD Acceleration</td><td>8-wide SIMD for dot products and magnitudes</td></tr>
<tr><td>Dual Search Modes</td><td>Brute-force O(n) or HNSW O(log n)</td></tr>
<tr><td>Unified Entities</td><td>Embeddings can be attached to shared entities</td></tr>
<tr><td>Thread Safety</td><td>Inherits from Tensor Store</td></tr>
<tr><td>Serializable Types</td><td>All types implement <code>serde::Serialize</code>/<code>Deserialize</code></td></tr>
<tr><td>Automatic Sparsity Detection</td><td>Vectors with &gt;50% zeros stored efficiently</td></tr>
</tbody></table>
</div>
<h2 id="architecture"><a class="header" href="#architecture">Architecture</a></h2>
<pre class="mermaid">graph TB
    subgraph VectorEngine
        VE[VectorEngine]
        SR[SearchResult]
        DM[DistanceMetric]
        VE --&gt; |uses| SR
        VE --&gt; |uses| DM
    end

    subgraph TensorStore
        TS[TensorStore]
        HNSW[HNSWIndex]
        SV[SparseVector]
        SIMD[SIMD Functions]
        ES[EmbeddingStorage]
    end

    VE --&gt; |stores to| TS
    VE --&gt; |builds| HNSW
    VE --&gt; |uses| SV
    VE --&gt; |uses| SIMD

    subgraph Storage
        EMB[&quot;emb:{key}&quot;]
        ENT[&quot;entity:{key}._embedding&quot;]
    end

    TS --&gt; EMB
    TS --&gt; ENT
</pre>
<h2 id="key-types"><a class="header" href="#key-types">Key Types</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody>
<tr><td><code>VectorEngine</code></td><td>Main engine for storing and searching embeddings</td></tr>
<tr><td><code>SearchResult</code></td><td>Result with key and similarity score</td></tr>
<tr><td><code>DistanceMetric</code></td><td>Enum: <code>Cosine</code>, <code>Euclidean</code>, <code>DotProduct</code></td></tr>
<tr><td><code>VectorError</code></td><td>Error types for vector operations</td></tr>
<tr><td><code>HNSWIndex</code></td><td>Hierarchical navigable small world graph (re-exported from tensor_store)</td></tr>
<tr><td><code>HNSWConfig</code></td><td>HNSW index configuration (re-exported from tensor_store)</td></tr>
<tr><td><code>SparseVector</code></td><td>Memory-efficient sparse embedding storage</td></tr>
<tr><td><code>EmbeddingStorage</code></td><td>Union type: Dense, Sparse, Delta, or TensorTrain</td></tr>
</tbody></table>
</div>
<h3 id="vectorerror-variants"><a class="header" href="#vectorerror-variants">VectorError Variants</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Variant</th><th>Description</th><th>When Triggered</th></tr></thead><tbody>
<tr><td><code>NotFound</code></td><td>Embedding key doesn’t exist</td><td><code>get_embedding</code>, <code>delete_embedding</code></td></tr>
<tr><td><code>DimensionMismatch</code></td><td>Vectors have different dimensions</td><td><code>compute_similarity</code> with mismatched inputs</td></tr>
<tr><td><code>EmptyVector</code></td><td>Empty vector provided</td><td>Any operation with <code>vec![]</code></td></tr>
<tr><td><code>InvalidTopK</code></td><td>top_k is 0</td><td><code>search_similar</code>, <code>search_with_hnsw</code></td></tr>
<tr><td><code>StorageError</code></td><td>Underlying Tensor Store error</td><td>Storage failures</td></tr>
</tbody></table>
</div>
<h2 id="distance-metrics"><a class="header" href="#distance-metrics">Distance Metrics</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Metric</th><th>Formula</th><th>Score Range</th><th>Use Case</th><th>HNSW Support</th></tr></thead><tbody>
<tr><td>Cosine</td><td><code>a.b / (‖a‖ * ‖b‖)</code></td><td>-1.0 to 1.0</td><td>Semantic similarity</td><td>Yes</td></tr>
<tr><td>Euclidean</td><td><code>1 / (1 + sqrt(sum((a-b)^2)))</code></td><td>0.0 to 1.0</td><td>Spatial distance</td><td>No (brute-force)</td></tr>
<tr><td>DotProduct</td><td><code>sum(a * b)</code></td><td>unbounded</td><td>Magnitude-aware</td><td>No (brute-force)</td></tr>
</tbody></table>
</div>
<p>All metrics return higher scores for better matches. Euclidean distance is
transformed to similarity score.</p>
<h3 id="distance-metric-implementation-details"><a class="header" href="#distance-metric-implementation-details">Distance Metric Implementation Details</a></h3>
<pre class="mermaid">flowchart TD
    Query[Query Vector] --&gt; MetricCheck{Which Metric?}

    MetricCheck --&gt;|Cosine| CosMag[Pre-compute query magnitude]
    CosMag --&gt; CosDot[SIMD dot product]
    CosDot --&gt; CosDiv[Divide by magnitudes]
    CosDiv --&gt; CosScore[Score: dot / mag_a * mag_b]

    MetricCheck --&gt;|Euclidean| EucDiff[Compute differences]
    EucDiff --&gt; EucSum[Sum of squares]
    EucSum --&gt; EucSqrt[Square root]
    EucSqrt --&gt; EucScore[Score: 1 / 1 + distance]

    MetricCheck --&gt;|DotProduct| DotSIMD[SIMD dot product]
    DotSIMD --&gt; DotScore[Score: raw dot product]
</pre>
<h4 id="cosine-similarity-edge-cases"><a class="header" href="#cosine-similarity-edge-cases">Cosine Similarity Edge Cases</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Zero-magnitude vectors return 0.0 similarity
let zero = vec![0.0, 0.0, 0.0];
let normal = vec![1.0, 2.0, 3.0];
VectorEngine::compute_similarity(&amp;zero, &amp;normal)?; // Returns 0.0

// Identical vectors return 1.0
VectorEngine::compute_similarity(&amp;normal, &amp;normal)?; // Returns 1.0

// Opposite vectors return -1.0
let opposite = vec![-1.0, -2.0, -3.0];
VectorEngine::compute_similarity(&amp;normal, &amp;opposite)?; // Returns -1.0

// Orthogonal vectors return 0.0
let a = vec![1.0, 0.0];
let b = vec![0.0, 1.0];
VectorEngine::compute_similarity(&amp;a, &amp;b)?; // Returns 0.0
<span class="boring">}</span></code></pre></pre>
<h4 id="euclidean-distance-transformation"><a class="header" href="#euclidean-distance-transformation">Euclidean Distance Transformation</a></h4>
<p>The engine transforms Euclidean distance to similarity score using <code>1 / (1 + distance)</code>:</p>
<div class="table-wrapper"><table><thead><tr><th>Distance</th><th>Similarity Score</th></tr></thead><tbody>
<tr><td>0.0</td><td>1.0 (identical)</td></tr>
<tr><td>1.0</td><td>0.5</td></tr>
<tr><td>2.0</td><td>0.333</td></tr>
<tr><td>9.0</td><td>0.1</td></tr>
<tr><td>Infinity</td><td>0.0</td></tr>
</tbody></table>
</div>
<h2 id="simd-implementation"><a class="header" href="#simd-implementation">SIMD Implementation</a></h2>
<p>The Vector Engine uses 8-wide SIMD operations via the <code>wide</code> crate for
accelerated distance computations.</p>
<h3 id="simd-dot-product-algorithm"><a class="header" href="#simd-dot-product-algorithm">SIMD Dot Product Algorithm</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Simplified view of the SIMD implementation
pub fn dot_product(a: &amp;[f32], b: &amp;[f32]) -&gt; f32 {
    let chunks = a.len() / 8;        // Process 8 floats at a time
    let remainder = a.len() % 8;

    let mut sum = f32x8::ZERO;

    // Process 8 elements at a time with SIMD
    for i in 0..chunks {
        let offset = i * 8;
        let va = f32x8::from(&amp;a[offset..offset + 8]);
        let vb = f32x8::from(&amp;b[offset..offset + 8]);
        sum += va * vb;  // Parallel multiply-add
    }

    // Sum SIMD lanes + handle remainder scalar
    let arr: [f32; 8] = sum.into();
    let mut result: f32 = arr.iter().sum();

    // Handle remainder with scalar operations
    let start = chunks * 8;
    for i in 0..remainder {
        result += a[start + i] * b[start + i];
    }

    result
}
<span class="boring">}</span></code></pre></pre>
<h3 id="simd-performance-characteristics"><a class="header" href="#simd-performance-characteristics">SIMD Performance Characteristics</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Dimension</th><th>SIMD Speedup</th><th>Notes</th></tr></thead><tbody>
<tr><td>8</td><td>1x</td><td>Baseline (single SIMD operation)</td></tr>
<tr><td>64</td><td>4-6x</td><td>Full pipeline utilization</td></tr>
<tr><td>384</td><td>6-8x</td><td>Sentence Transformers size</td></tr>
<tr><td>768</td><td>6-8x</td><td>BERT embedding size</td></tr>
<tr><td>1536</td><td>6-8x</td><td>OpenAI ada-002 size</td></tr>
<tr><td>3072</td><td>6-8x</td><td>OpenAI text-embedding-3-large</td></tr>
</tbody></table>
</div>
<p>SIMD operations are cache-friendly due to sequential memory access patterns.</p>
<h2 id="api-reference"><a class="header" href="#api-reference">API Reference</a></h2>
<h3 id="basic-operations"><a class="header" href="#basic-operations">Basic Operations</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let engine = VectorEngine::new();

// Store an embedding
engine.store_embedding("doc1", vec![0.1, 0.2, 0.3])?;

// Get an embedding
let vector = engine.get_embedding("doc1")?;

// Delete an embedding
engine.delete_embedding("doc1")?;

// Check existence
engine.exists("doc1");  // -&gt; bool

// Count embeddings
engine.count();  // -&gt; usize

// List all keys
let keys = engine.list_keys();

// Clear all embeddings
engine.clear()?;

// Get dimension (from first embedding)
engine.dimension();  // -&gt; Option&lt;usize&gt;
<span class="boring">}</span></code></pre></pre>
<h3 id="similarity-search"><a class="header" href="#similarity-search">Similarity Search</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Find top-k most similar (cosine by default)
let query = vec![0.1, 0.2, 0.3];
let results = engine.search_similar(&amp;query, 5)?;

for result in results {
    println!("Key: {}, Score: {}", result.key, result.score);
}

// Search with specific metric
let results = engine.search_similar_with_metric(
    &amp;query,
    5,
    DistanceMetric::Euclidean
)?;

// Direct similarity computation
let similarity = VectorEngine::compute_similarity(&amp;vec_a, &amp;vec_b)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="search-flow-diagram"><a class="header" href="#search-flow-diagram">Search Flow Diagram</a></h3>
<pre class="mermaid">sequenceDiagram
    participant Client
    participant VE as VectorEngine
    participant TS as TensorStore
    participant SIMD

    Client-&gt;&gt;VE: search_similar(query, k)
    VE-&gt;&gt;VE: Validate query (non-empty, k &gt; 0)
    VE-&gt;&gt;SIMD: Pre-compute query magnitude
    VE-&gt;&gt;TS: scan(&quot;emb:&quot;)
    TS--&gt;&gt;VE: List of embedding keys

    alt Dataset &lt; 5000 vectors
        VE-&gt;&gt;VE: Sequential search
    else Dataset &gt;= 5000 vectors
        VE-&gt;&gt;VE: Parallel search (rayon)
    end

    loop For each embedding
        VE-&gt;&gt;TS: get(key)
        TS--&gt;&gt;VE: TensorData
        VE-&gt;&gt;VE: Extract vector (dense or sparse)
        VE-&gt;&gt;SIMD: cosine_similarity(query, stored)
        VE-&gt;&gt;VE: Collect SearchResult
    end

    VE-&gt;&gt;VE: Sort by score descending
    VE-&gt;&gt;VE: Truncate to top k
    VE--&gt;&gt;Client: Vec&lt;SearchResult&gt;
</pre>
<h3 id="hnsw-index"><a class="header" href="#hnsw-index">HNSW Index</a></h3>
<p>For large datasets, build an HNSW index for O(log n) search:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Build index with default config
let (index, key_mapping) = engine.build_hnsw_index_default()?;

// Search using the index
let results = engine.search_with_hnsw(&amp;index, &amp;key_mapping, &amp;query, 10)?;

// Build with custom config
let config = HNSWConfig::high_recall();
let (index, key_mapping) = engine.build_hnsw_index(config)?;

// Direct HNSW operations
let index = HNSWIndex::new();
index.insert(vec![1.0, 2.0, 3.0]);
let results = index.search(&amp;query, 10);

// Search with custom ef (recall/speed tradeoff)
let results = index.search_with_ef(&amp;query, 10, 200);
<span class="boring">}</span></code></pre></pre>
<h3 id="hnsw-search-flow"><a class="header" href="#hnsw-search-flow">HNSW Search Flow</a></h3>
<pre class="mermaid">flowchart TD
    Query[Query Vector] --&gt; Entry[Entry Point at Max Layer]

    Entry --&gt; Greedy1[Greedy Search Layer L]
    Greedy1 --&gt; |Find closest| Greedy2[Greedy Search Layer L-1]
    Greedy2 --&gt; |...|GreedyN[Greedy Search until Layer 1]

    GreedyN --&gt; Layer0[Full ef-Search at Layer 0]

    Layer0 --&gt; Candidates[Candidate Pool]
    Candidates --&gt; |BinaryHeap min-heap| Visit[Visit Neighbors]
    Visit --&gt; Distance[Compute Distances]
    Distance --&gt; |Update| Results[Result Pool]
    Results --&gt; |BinaryHeap max-heap| Prune[Keep top ef]

    Prune --&gt; |More candidates?| Visit
    Prune --&gt; |Done| TopK[Return Top K]
</pre>
<h3 id="unified-entity-mode"><a class="header" href="#unified-entity-mode">Unified Entity Mode</a></h3>
<p>Attach embeddings directly to entities for cross-engine queries:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let store = TensorStore::new();
let engine = VectorEngine::with_store(store.clone());

// Set embedding on an entity
engine.set_entity_embedding("user:1", vec![0.1, 0.2, 0.3])?;

// Get embedding from an entity
let embedding = engine.get_entity_embedding("user:1")?;

// Check if entity has embedding
engine.entity_has_embedding("user:1");  // -&gt; bool

// Remove embedding (preserves other entity data)
engine.remove_entity_embedding("user:1")?;

// Search entities with embeddings
let results = engine.search_entities(&amp;query, 5)?;

// Scan all entities with embeddings
let entity_keys = engine.scan_entities_with_embeddings();

// Count entities with embeddings
let count = engine.count_entities_with_embeddings();
<span class="boring">}</span></code></pre></pre>
<p>Unified entity embeddings are stored in the <code>_embedding</code> field of the entity’s
TensorData.</p>
<h2 id="storage-model"><a class="header" href="#storage-model">Storage Model</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Key Pattern</th><th>Content</th><th>Use Case</th></tr></thead><tbody>
<tr><td><code>emb:{key}</code></td><td>TensorData with “vector” field</td><td>Standalone embeddings</td></tr>
<tr><td><code>{entity_key}</code></td><td>TensorData with “_embedding” field</td><td>Unified entities</td></tr>
</tbody></table>
</div>
<h3 id="automatic-sparse-storage"><a class="header" href="#automatic-sparse-storage">Automatic Sparse Storage</a></h3>
<p>Vectors with &gt;50% zeros are automatically stored as sparse vectors:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Detection threshold: nnz * 2 &lt;= len (i.e., sparsity &gt;= 50%)
fn should_use_sparse(vector: &amp;[f32]) -&gt; bool {
    let nnz = vector.iter().filter(|&amp;&amp;v| v.abs() &gt; 1e-6).count();
    nnz * 2 &lt;= vector.len()
}

// 97% sparse vector (3 non-zeros in 100 elements)
let mut sparse = vec![0.0f32; 100];
sparse[0] = 1.0;
sparse[50] = 2.0;
sparse[99] = 3.0;

// Stored efficiently as SparseVector
engine.store_embedding("sparse_doc", sparse)?;

// Retrieved as dense for computation
let dense = engine.get_embedding("sparse_doc")?;
<span class="boring">}</span></code></pre></pre>
<h3 id="storage-format-comparison"><a class="header" href="#storage-format-comparison">Storage Format Comparison</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Format</th><th>Memory per Element</th><th>Best For</th></tr></thead><tbody>
<tr><td>Dense</td><td>4 bytes</td><td>Sparsity &lt; 50%</td></tr>
<tr><td>Sparse</td><td>8 bytes per non-zero (4 pos + 4 val)</td><td>Sparsity &gt; 50%</td></tr>
</tbody></table>
</div>
<p>Example: 1000-dim vector with 100 non-zeros:</p>
<ul>
<li>Dense: 4000 bytes</li>
<li>Sparse: 800 bytes (5x compression)</li>
</ul>
<h2 id="sparse-vector-operations"><a class="header" href="#sparse-vector-operations">Sparse Vector Operations</a></h2>
<h3 id="memory-layout"><a class="header" href="#memory-layout">Memory Layout</a></h3>
<pre><code class="language-text">SparseVector {
    dimension: usize,        // Total vector dimension
    positions: Vec&lt;u32&gt;,     // Sorted indices of non-zeros
    values: Vec&lt;f32&gt;,        // Corresponding values
}
</code></pre>
<h3 id="sparse-dot-product-algorithm"><a class="header" href="#sparse-dot-product-algorithm">Sparse Dot Product Algorithm</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// O(min(nnz_a, nnz_b)) - only overlapping positions contribute
pub fn dot(&amp;self, other: &amp;SparseVector) -&gt; f32 {
    let mut result = 0.0;
    let mut i = 0;
    let mut j = 0;

    // Merge-sort style traversal
    while i &lt; self.positions.len() &amp;&amp; j &lt; other.positions.len() {
        match self.positions[i].cmp(&amp;other.positions[j]) {
            Equal =&gt; {
                result += self.values[i] * other.values[j];
                i += 1; j += 1;
            },
            Less =&gt; i += 1,
            Greater =&gt; j += 1,
        }
    }
    result
}
<span class="boring">}</span></code></pre></pre>
<h3 id="sparse-dense-dot-product"><a class="header" href="#sparse-dense-dot-product">Sparse-Dense Dot Product</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// O(nnz) - only iterate over sparse non-zeros
pub fn dot_dense(&amp;self, dense: &amp;[f32]) -&gt; f32 {
    self.positions.iter()
        .zip(&amp;self.values)
        .map(|(&amp;pos, &amp;val)| val * dense[pos as usize])
        .sum()
}
<span class="boring">}</span></code></pre></pre>
<h3 id="sparse-distance-metrics"><a class="header" href="#sparse-distance-metrics">Sparse Distance Metrics</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Metric</th><th>Complexity</th><th>Description</th></tr></thead><tbody>
<tr><td><code>dot</code></td><td>O(min(nnz_a, nnz_b))</td><td>Sparse-sparse dot product</td></tr>
<tr><td><code>dot_dense</code></td><td>O(nnz)</td><td>Sparse-dense dot product</td></tr>
<tr><td><code>cosine_similarity</code></td><td>O(min(nnz_a, nnz_b))</td><td>Angle-based similarity</td></tr>
<tr><td><code>euclidean_distance</code></td><td>O(nnz_a + nnz_b)</td><td>L2 distance</td></tr>
<tr><td><code>manhattan_distance</code></td><td>O(nnz_a + nnz_b)</td><td>L1 distance</td></tr>
<tr><td><code>jaccard_index</code></td><td>O(min(nnz_a, nnz_b))</td><td>Position overlap</td></tr>
<tr><td><code>angular_distance</code></td><td>O(min(nnz_a, nnz_b))</td><td>Arc-cosine</td></tr>
</tbody></table>
</div>
<h2 id="hnsw-configuration"><a class="header" href="#hnsw-configuration">HNSW Configuration</a></h2>
<h3 id="configuration-parameters"><a class="header" href="#configuration-parameters">Configuration Parameters</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Parameter</th><th>Default</th><th>Description</th></tr></thead><tbody>
<tr><td><code>m</code></td><td>16</td><td>Max connections per node per layer</td></tr>
<tr><td><code>m0</code></td><td>32</td><td>Max connections at layer 0 (2*m)</td></tr>
<tr><td><code>ef_construction</code></td><td>200</td><td>Candidates during index building</td></tr>
<tr><td><code>ef_search</code></td><td>50</td><td>Candidates during search</td></tr>
<tr><td><code>ml</code></td><td>1/ln(m)</td><td>Level multiplier for layer selection</td></tr>
<tr><td><code>sparsity_threshold</code></td><td>0.5</td><td>Auto-sparse threshold</td></tr>
<tr><td><code>max_nodes</code></td><td>10,000,000</td><td>Capacity limit</td></tr>
</tbody></table>
</div>
<h3 id="presets"><a class="header" href="#presets">Presets</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Preset</th><th>m</th><th>m0</th><th>ef_construction</th><th>ef_search</th><th>Use Case</th></tr></thead><tbody>
<tr><td><code>default()</code></td><td>16</td><td>32</td><td>200</td><td>50</td><td>Balanced</td></tr>
<tr><td><code>high_recall()</code></td><td>32</td><td>64</td><td>400</td><td>200</td><td>Accuracy over speed</td></tr>
<tr><td><code>high_speed()</code></td><td>8</td><td>16</td><td>100</td><td>20</td><td>Speed over accuracy</td></tr>
</tbody></table>
</div>
<h3 id="tuning-guidelines"><a class="header" href="#tuning-guidelines">Tuning Guidelines</a></h3>
<pre class="mermaid">graph TD
    subgraph &quot;Higher m / ef&quot;
        A[More connections per node]
        B[Better recall]
        C[More memory]
        D[Slower insert]
    end

    subgraph &quot;Lower m / ef&quot;
        E[Fewer connections]
        F[Lower recall]
        G[Less memory]
        H[Faster insert]
    end

    A --&gt; B
    A --&gt; C
    A --&gt; D

    E --&gt; F
    E --&gt; G
    E --&gt; H
</pre>
<h4 id="workload-specific-tuning"><a class="header" href="#workload-specific-tuning">Workload-Specific Tuning</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Workload</th><th>Recommended Config</th><th>Rationale</th></tr></thead><tbody>
<tr><td>RAG/Semantic Search</td><td><code>high_recall()</code></td><td>Accuracy critical</td></tr>
<tr><td>Real-time recommendations</td><td><code>high_speed()</code></td><td>Latency critical</td></tr>
<tr><td>Batch processing</td><td><code>default()</code></td><td>Balanced</td></tr>
<tr><td>Small dataset (&lt;10K)</td><td>Brute-force</td><td>HNSW overhead not worth it</td></tr>
<tr><td>Large dataset (&gt;100K)</td><td><code>default()</code> with higher ef_search</td><td>Scale benefits</td></tr>
</tbody></table>
</div>
<h4 id="memory-vs-recall-tradeoff"><a class="header" href="#memory-vs-recall-tradeoff">Memory vs Recall Tradeoff</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Config</th><th>Memory/Node</th><th>Recall@10</th><th>Search Time</th></tr></thead><tbody>
<tr><td>high_speed</td><td>~128 bytes</td><td>~85%</td><td>0.1ms</td></tr>
<tr><td>default</td><td>~256 bytes</td><td>~95%</td><td>0.3ms</td></tr>
<tr><td>high_recall</td><td>~512 bytes</td><td>~99%</td><td>1.0ms</td></tr>
</tbody></table>
</div>
<h2 id="performance-characteristics"><a class="header" href="#performance-characteristics">Performance Characteristics</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Complexity</th><th>Notes</th></tr></thead><tbody>
<tr><td><code>store_embedding</code></td><td>O(1)</td><td>Single store put</td></tr>
<tr><td><code>get_embedding</code></td><td>O(1)</td><td>Single store get</td></tr>
<tr><td><code>delete_embedding</code></td><td>O(1)</td><td>Single store delete</td></tr>
<tr><td><code>search_similar</code></td><td>O(n*d)</td><td>Brute-force, n=count, d=dimension</td></tr>
<tr><td><code>search_with_hnsw</code></td><td>O(log n <em>ef</em> m)</td><td>Approximate nearest neighbor</td></tr>
<tr><td><code>build_hnsw_index</code></td><td>O(n <em>log n</em> ef_construction * m)</td><td>Index construction</td></tr>
<tr><td><code>count</code></td><td>O(n)</td><td>Scans all embeddings</td></tr>
<tr><td><code>list_keys</code></td><td>O(n)</td><td>Scans all embeddings</td></tr>
</tbody></table>
</div>
<h3 id="parallel-search-threshold"><a class="header" href="#parallel-search-threshold">Parallel Search Threshold</a></h3>
<p>Automatic parallel iteration for datasets &gt;5000 vectors:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>const PARALLEL_THRESHOLD: usize = 5000;

if keys.len() &gt;= PARALLEL_THRESHOLD {
    // Use rayon parallel iterator
    keys.par_iter().filter_map(...)
} else {
    // Use sequential iterator
    keys.iter().filter_map(...)
}
<span class="boring">}</span></code></pre></pre>
<h3 id="benchmark-results"><a class="header" href="#benchmark-results">Benchmark Results</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Dataset Size</th><th>Brute-Force</th><th>With HNSW</th><th>Speedup</th></tr></thead><tbody>
<tr><td>200 vectors</td><td>4.17s</td><td>9.3us</td><td>448,000x</td></tr>
<tr><td>1,000 vectors</td><td>~5ms</td><td>~20us</td><td>250x</td></tr>
<tr><td>10,000 vectors</td><td>~50ms</td><td>~50us</td><td>1000x</td></tr>
<tr><td>100,000 vectors</td><td>~500ms</td><td>~100us</td><td>5000x</td></tr>
</tbody></table>
</div>
<h2 id="supported-embedding-dimensions"><a class="header" href="#supported-embedding-dimensions">Supported Embedding Dimensions</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Model</th><th>Dimensions</th><th>Recommended Config</th></tr></thead><tbody>
<tr><td>OpenAI text-embedding-ada-002</td><td>1536</td><td>default</td></tr>
<tr><td>OpenAI text-embedding-3-small</td><td>1536</td><td>default</td></tr>
<tr><td>OpenAI text-embedding-3-large</td><td>3072</td><td>high_recall</td></tr>
<tr><td>BERT base</td><td>768</td><td>default</td></tr>
<tr><td>Sentence Transformers</td><td>384-768</td><td>default</td></tr>
<tr><td>Cohere embed-v3</td><td>1024</td><td>default</td></tr>
<tr><td>Custom/small</td><td>&lt;256</td><td>high_speed</td></tr>
</tbody></table>
</div>
<h2 id="edge-cases-and-gotchas"><a class="header" href="#edge-cases-and-gotchas">Edge Cases and Gotchas</a></h2>
<h3 id="zero-magnitude-vectors"><a class="header" href="#zero-magnitude-vectors">Zero-Magnitude Vectors</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Metric</th><th>Behavior</th><th>Rationale</th></tr></thead><tbody>
<tr><td>Cosine</td><td>Returns empty results</td><td>Division by zero undefined</td></tr>
<tr><td>DotProduct</td><td>Returns empty results</td><td>Undefined direction</td></tr>
<tr><td>Euclidean</td><td>Works correctly</td><td>Finds vectors closest to origin</td></tr>
</tbody></table>
</div>
<h3 id="dimension-mismatch-handling"><a class="header" href="#dimension-mismatch-handling">Dimension Mismatch Handling</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Mismatched dimensions are silently skipped during search
engine.store_embedding("2d", vec![1.0, 0.0])?;
engine.store_embedding("3d", vec![1.0, 0.0, 0.0])?;

// Search with 2D query only matches 2D vectors
let results = engine.search_similar(&amp;[1.0, 0.0], 10)?;
assert_eq!(results.len(), 1);  // Only "2d" matched
<span class="boring">}</span></code></pre></pre>
<h3 id="hnsw-limitations"><a class="header" href="#hnsw-limitations">HNSW Limitations</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Limitation</th><th>Details</th><th>Workaround</th></tr></thead><tbody>
<tr><td>Only cosine similarity</td><td>HNSW uses cosine distance internally</td><td>Use brute-force for other metrics</td></tr>
<tr><td>No deletion</td><td>Cannot remove vectors</td><td>Rebuild index</td></tr>
<tr><td>Static after build</td><td>Index doesn’t update with new vectors</td><td>Rebuild periodically</td></tr>
<tr><td>Memory overhead</td><td>Graph structure adds ~2-4x</td><td>Use for large datasets only</td></tr>
</tbody></table>
</div>
<h3 id="naninfinity-handling"><a class="header" href="#naninfinity-handling">NaN/Infinity Handling</a></h3>
<p>Sparse vector operations sanitize NaN/Inf results:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// cosine_similarity returns 0.0 for NaN/Inf
if result.is_nan() || result.is_infinite() {
    0.0
} else {
    result.clamp(-1.0, 1.0)
}

// cosine_distance_dense returns 1.0 (max distance) for NaN/Inf
if similarity.is_nan() || similarity.is_infinite() {
    1.0  // Maximum distance
} else {
    1.0 - similarity.clamp(-1.0, 1.0)
}
<span class="boring">}</span></code></pre></pre>
<h2 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h2>
<h3 id="memory-optimization"><a class="header" href="#memory-optimization">Memory Optimization</a></h3>
<ol>
<li><strong>Use sparse vectors for high-sparsity data</strong>: Automatic at &gt;50% zeros</li>
<li><strong>Batch insert for HNSW</strong>: Build index once after all data loaded</li>
<li><strong>Choose appropriate HNSW config</strong>: Don’t over-provision m/ef</li>
<li><strong>Monitor memory with <code>HNSWMemoryStats</code></strong>: Track dense vs sparse counts</li>
</ol>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let stats = index.memory_stats();
println!("Dense: {}, Sparse: {}, Total bytes: {}",
    stats.dense_count, stats.sparse_count, stats.embedding_bytes);
<span class="boring">}</span></code></pre></pre>
<h3 id="search-performance"><a class="header" href="#search-performance">Search Performance</a></h3>
<ol>
<li><strong>Pre-compute query magnitude</strong>: Done automatically in search</li>
<li><strong>Use HNSW for &gt;10K vectors</strong>: Brute-force for smaller sets</li>
<li><strong>Tune ef_search</strong>: Higher for recall, lower for speed</li>
<li><strong>Parallel threshold</strong>: Automatic at 5000 vectors</li>
</ol>
<h3 id="unified-entity-best-practices"><a class="header" href="#unified-entity-best-practices">Unified Entity Best Practices</a></h3>
<ol>
<li><strong>Use for cross-engine queries</strong>: When embeddings relate to graph/relational
data</li>
<li><strong>Entity key conventions</strong>: Use prefixes like <code>user:</code>, <code>doc:</code>, <code>item:</code></li>
<li><strong>Separate embedding namespace</strong>: Use <code>store_embedding</code> for isolated vectors</li>
</ol>
<h2 id="dependencies"><a class="header" href="#dependencies">Dependencies</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Crate</th><th>Purpose</th></tr></thead><tbody>
<tr><td><code>tensor_store</code></td><td>Persistence, SparseVector, HNSWIndex, SIMD</td></tr>
<tr><td><code>rayon</code></td><td>Parallel iteration for large datasets</td></tr>
<tr><td><code>serde</code></td><td>Serialization of types</td></tr>
<tr><td><code>wide</code></td><td>SIMD f32x8 operations</td></tr>
</tbody></table>
</div>
<h2 id="related-modules"><a class="header" href="#related-modules">Related Modules</a></h2>
<ul>
<li><a href="tensor-store.html">Tensor Store</a> - Underlying storage and HNSW implementation</li>
<li><a href="query-router.html">Query Router</a> - Executes SIMILAR queries using VectorEngine</li>
<li><a href="tensor-cache.html">Tensor Cache</a> - Uses vector similarity for semantic caching</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../architecture/graph-engine.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../architecture/tensor-compress.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../architecture/graph-engine.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../architecture/tensor-compress.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>



        <script>
            window.playground_line_numbers = true;
        </script>

        <script>
            window.playground_copyable = true;
        </script>

        <script src="../ace.js"></script>
        <script src="../mode-rust.js"></script>
        <script src="../editor.js"></script>
        <script src="../theme-dawn.js"></script>
        <script src="../theme-tomorrow_night.js"></script>

        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
