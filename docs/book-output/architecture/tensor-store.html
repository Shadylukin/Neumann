<!DOCTYPE HTML>
<html lang="en" class="rust sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Tensor Store - Neumann</title>


        <!-- Custom HTML head -->

        <meta name="description" content="Unified tensor-based runtime for relational, graph, and vector data">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "rust";
            const default_dark_theme = "coal";
            window.path_to_searchindex_js = "../searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('rust')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Neumann</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/Shadylukin/Neumann" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        <a href="https://github.com/Shadylukin/Neumann/edit/main/docs/book/src/architecture/tensor-store.md" title="Suggest an edit" aria-label="Suggest an edit" rel="edit">
                            <i id="git-edit-button" class="fa fa-edit"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="tensor-store-architecture"><a class="header" href="#tensor-store-architecture">Tensor Store Architecture</a></h1>
<p>The tensor_store crate is the foundational storage layer for Neumann. It
provides a unified tensor-based key-value store that holds all data -
relational, graph, and vector - in a single mathematical structure. The store
knows nothing about queries; it purely stores and retrieves tensors by key.</p>
<p>The architecture uses SlabRouter internally, which routes operations to
specialized slabs based on key prefixes. This design eliminates hash table
resize stalls by using BTreeMap-based storage, providing predictable O(log n)
performance without the throughput cliffs caused by hash map resizing.</p>
<h2 id="core-types"><a class="header" href="#core-types">Core Types</a></h2>
<h3 id="tensorvalue"><a class="header" href="#tensorvalue">TensorValue</a></h3>
<p>Represents different types of values a tensor can hold.</p>
<div class="table-wrapper"><table><thead><tr><th>Variant</th><th>Rust Type</th><th>Use Case</th></tr></thead><tbody>
<tr><td><code>Scalar(ScalarValue)</code></td><td>enum</td><td>Properties (name, age, active)</td></tr>
<tr><td><code>Vector(Vec&lt;f32&gt;)</code></td><td>dense array</td><td>Embeddings for similarity search</td></tr>
<tr><td><code>Sparse(SparseVector)</code></td><td>compressed</td><td>Sparse embeddings (&gt;70% zeros)</td></tr>
<tr><td><code>Pointer(String)</code></td><td>single ref</td><td>Single relationship to another tensor</td></tr>
<tr><td><code>Pointers(Vec&lt;String&gt;)</code></td><td>multi ref</td><td>Multiple relationships</td></tr>
</tbody></table>
</div>
<p><strong>Automatic Sparsification</strong>: Use <code>TensorValue::from_embedding_auto(dense)</code> to
automatically choose between dense and sparse representation based on sparsity:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Automatically uses Sparse if sparsity &gt;= 70%
let val = TensorValue::from_embedding_auto(dense_vec);

// With custom thresholds (value_threshold, sparsity_threshold)
let val = TensorValue::from_embedding(dense_vec, 0.01, 0.8);
<span class="boring">}</span></code></pre></pre>
<p><strong>Vector Operations</strong>: TensorValue supports cross-format operations:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Dot product works across Dense, Sparse, and mixed
let dot = tensor_a.dot(&amp;tensor_b);

// Cosine similarity with automatic format handling
let sim = tensor_a.cosine_similarity(&amp;tensor_b);
<span class="boring">}</span></code></pre></pre>
<h3 id="scalarvalue"><a class="header" href="#scalarvalue">ScalarValue</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Variant</th><th>Rust Type</th><th>Example</th></tr></thead><tbody>
<tr><td><code>Null</code></td><td>—</td><td>Missing/undefined value</td></tr>
<tr><td><code>Bool</code></td><td><code>bool</code></td><td><code>true</code>, <code>false</code></td></tr>
<tr><td><code>Int</code></td><td><code>i64</code></td><td><code>42</code>, <code>-1</code></td></tr>
<tr><td><code>Float</code></td><td><code>f64</code></td><td><code>3.14159</code></td></tr>
<tr><td><code>String</code></td><td><code>String</code></td><td><code>"Alice"</code></td></tr>
<tr><td><code>Bytes</code></td><td><code>Vec&lt;u8&gt;</code></td><td>Raw binary data</td></tr>
</tbody></table>
</div>
<h3 id="tensordata"><a class="header" href="#tensordata">TensorData</a></h3>
<p>An entity that holds scalar properties, vector embeddings, and pointers to other
tensors via a <code>HashMap&lt;String, TensorValue&gt;</code> internally.</p>
<h3 id="reserved-field-names"><a class="header" href="#reserved-field-names">Reserved Field Names</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Purpose</th><th>Used By</th></tr></thead><tbody>
<tr><td><code>_out</code></td><td>Outgoing graph edge pointers</td><td>GraphEngine</td></tr>
<tr><td><code>_in</code></td><td>Incoming graph edge pointers</td><td>GraphEngine</td></tr>
<tr><td><code>_embedding</code></td><td>Vector embedding</td><td>VectorEngine</td></tr>
<tr><td><code>_label</code></td><td>Entity type/label</td><td>GraphEngine</td></tr>
<tr><td><code>_type</code></td><td>Discriminator field</td><td>All engines</td></tr>
<tr><td><code>_from</code></td><td>Edge source</td><td>GraphEngine</td></tr>
<tr><td><code>_to</code></td><td>Edge target</td><td>GraphEngine</td></tr>
<tr><td><code>_edge_type</code></td><td>Edge relationship type</td><td>GraphEngine</td></tr>
<tr><td><code>_directed</code></td><td>Edge direction flag</td><td>GraphEngine</td></tr>
<tr><td><code>_table</code></td><td>Table membership</td><td>RelationalEngine</td></tr>
<tr><td><code>_id</code></td><td>Entity ID</td><td>System</td></tr>
</tbody></table>
</div>
<h2 id="architecture-diagram"><a class="header" href="#architecture-diagram">Architecture Diagram</a></h2>
<pre><code class="language-text">TensorStore
  |
  +-- Arc&lt;SlabRouter&gt;
         |
         +-- MetadataSlab (general key-value, BTreeMap-based)
         +-- EntityIndex (sorted vocabulary + hash index)
         +-- EmbeddingSlab (dense f32 arrays)
         +-- GraphTensor (CSR format for edges)
         +-- RelationalSlab (columnar storage)
         +-- CacheRing (LRU/LFU eviction)
         +-- BlobLog (append-only blob storage)
</code></pre>
<h2 id="slabrouter-internals"><a class="header" href="#slabrouter-internals">SlabRouter Internals</a></h2>
<p>SlabRouter is the core routing layer that directs operations to specialized
storage backends based on key prefixes.</p>
<h3 id="key-routing-algorithm"><a class="header" href="#key-routing-algorithm">Key Routing Algorithm</a></h3>
<pre class="mermaid">flowchart TD
    A[put/get/delete key] --&gt; B{Classify Key}
    B --&gt;|emb:*| C[EmbeddingSlab + MetadataSlab]
    B --&gt;|node:* / edge:*| D[GraphTensor via MetadataSlab]
    B --&gt;|table:*| E[RelationalSlab via MetadataSlab]
    B --&gt;|_cache:*| F[CacheRing]
    B --&gt;|Everything else| G[MetadataSlab]
</pre>
<h3 id="key-classification"><a class="header" href="#key-classification">Key Classification</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Prefix</th><th>KeyClass</th><th>Slab</th><th>Purpose</th></tr></thead><tbody>
<tr><td><code>emb:*</code></td><td>Embedding</td><td>EmbeddingSlab + EntityIndex</td><td>Embedding vectors with stable ID assignment</td></tr>
<tr><td><code>node:*</code>, <code>edge:*</code></td><td>Graph</td><td>MetadataSlab</td><td>Graph nodes and edges</td></tr>
<tr><td><code>table:*</code></td><td>Table</td><td>MetadataSlab</td><td>Relational rows</td></tr>
<tr><td><code>_cache:*</code></td><td>Cache</td><td>CacheRing</td><td>Cached data with eviction</td></tr>
<tr><td><code>_blob:*</code></td><td>Metadata</td><td>MetadataSlab</td><td>Blob metadata (chunks stored separately)</td></tr>
<tr><td>Everything else</td><td>Metadata</td><td>MetadataSlab</td><td>General key-value storage</td></tr>
</tbody></table>
</div>
<h3 id="slabrouter-operation-flow"><a class="header" href="#slabrouter-operation-flow">SlabRouter Operation Flow</a></h3>
<p><strong>PUT Operation</strong>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn put(&amp;self, key: &amp;str, value: TensorData) {
    match classify_key(key) {
        KeyClass::Embedding =&gt; {
            // 1. Get or create stable entity ID
            let entity_id = self.index.get_or_create(key);
            // 2. Extract and store embedding vector
            if let Some(TensorValue::Vector(vec)) = value.get("_embedding") {
                self.embeddings.set(entity_id, vec);
            }
            // 3. Store full metadata
            self.metadata.set(key, value);
        }
        KeyClass::Cache =&gt; {
            let size = estimate_size(&amp;value);
            self.cache.put(key, value, 1.0, size);
        }
        _ =&gt; self.metadata.set(key, value),
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>GET Operation</strong>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn get(&amp;self, key: &amp;str) -&gt; Result&lt;TensorData&gt; {
    match classify_key(key) {
        KeyClass::Embedding =&gt; {
            // Try to reconstruct from embedding slab + metadata
            if let Some(entity_id) = self.index.get(key) {
                if let Some(vector) = self.embeddings.get(entity_id) {
                    let mut data = self.metadata.get(key).unwrap_or_default();
                    data.set("_embedding", TensorValue::Vector(vector));
                    return Ok(data);
                }
            }
            self.metadata.get(key)
        }
        KeyClass::Cache =&gt; self.cache.get(key),
        _ =&gt; self.metadata.get(key),
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="specialized-slabs"><a class="header" href="#specialized-slabs">Specialized Slabs</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Slab</th><th>Data Structure</th><th>Purpose</th></tr></thead><tbody>
<tr><td><code>MetadataSlab</code></td><td><code>RwLock&lt;BTreeMap&lt;String, TensorData&gt;&gt;</code></td><td>General key-value storage</td></tr>
<tr><td><code>EntityIndex</code></td><td>Sorted vocabulary + hash index</td><td>Stable ID assignment</td></tr>
<tr><td><code>EmbeddingSlab</code></td><td>Dense f32 arrays + BTreeMap</td><td>Embedding vectors</td></tr>
<tr><td><code>GraphTensor</code></td><td>CSR format (row pointers + column indices)</td><td>Graph edges</td></tr>
<tr><td><code>RelationalSlab</code></td><td>Columnar storage</td><td>Table rows</td></tr>
<tr><td><code>CacheRing</code></td><td>Ring buffer with LRU/LFU</td><td>Fixed-size cache</td></tr>
<tr><td><code>BlobLog</code></td><td>Append-only segments</td><td>Large binary data</td></tr>
</tbody></table>
</div>
<h2 id="performance-characteristics"><a class="header" href="#performance-characteristics">Performance Characteristics</a></h2>
<h3 id="operation-complexity"><a class="header" href="#operation-complexity">Operation Complexity</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Time Complexity</th><th>Notes</th></tr></thead><tbody>
<tr><td><code>put</code></td><td>O(log n)</td><td>BTreeMap insert</td></tr>
<tr><td><code>get</code></td><td>O(log n) + clone</td><td>Clone prevents reference issues</td></tr>
<tr><td><code>delete</code></td><td>O(log n)</td><td>BTreeMap remove</td></tr>
<tr><td><code>exists</code></td><td>O(log n)</td><td>BTreeMap lookup</td></tr>
<tr><td><code>scan</code></td><td>O(k + log n)</td><td>BTreeMap range, k = result count</td></tr>
<tr><td><code>scan_count</code></td><td>O(k + log n)</td><td>No allocation</td></tr>
<tr><td><code>scan_filter_map</code></td><td>O(k + log n)</td><td>Single-pass filter with selective cloning</td></tr>
<tr><td><code>len</code></td><td>O(1)</td><td>Cached count</td></tr>
<tr><td><code>clear</code></td><td>O(n)</td><td>Clears all data</td></tr>
</tbody></table>
</div>
<h3 id="throughput-comparison"><a class="header" href="#throughput-comparison">Throughput Comparison</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Metric</th><th>SlabRouter</th><th>Previous (DashMap)</th></tr></thead><tbody>
<tr><td>PUT throughput</td><td>3.1+ M ops/sec</td><td>2.5 M ops/sec</td></tr>
<tr><td>GET throughput</td><td>4.9+ M ops/sec</td><td>4.5 M ops/sec</td></tr>
<tr><td>Throughput variance (CV)</td><td>12% steady-state</td><td>222% during resize</td></tr>
<tr><td>Resize stalls</td><td>None</td><td>99.6% throughput drops</td></tr>
</tbody></table>
</div>
<h3 id="optimized-scan-performance"><a class="header" href="#optimized-scan-performance">Optimized Scan Performance</a></h3>
<p>Use <code>scan_filter_map</code> for selective queries to avoid cloning non-matching
entries:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Old path: 5000 clones for 5000 rows, ~2.6ms
let users = store.scan("users:");
let matches: Vec&lt;_&gt; = users.iter()
    .filter_map(|key| store.get(key).ok())
    .filter(|data| /* condition */)
    .collect();

// New path: 250 clones for 5% match rate, ~0.13ms (20x faster)
let matches = store.scan_filter_map("users:", |key, data| {
    if /* condition */ {
        Some(data.clone())
    } else {
        None
    }
});
<span class="boring">}</span></code></pre></pre>
<h2 id="concurrency-model"><a class="header" href="#concurrency-model">Concurrency Model</a></h2>
<p>TensorStore uses tensor-based structures instead of hash maps for predictable
performance:</p>
<ul>
<li><strong>No Resize Stalls</strong>: BTreeMap and sorted arrays grow incrementally</li>
<li><strong>Lock-free Reads</strong>: RwLock allows many concurrent readers</li>
<li><strong>Predictable Writes</strong>: O(log n) inserts, no amortized O(n) resizing</li>
<li><strong>Clone on Read</strong>: <code>get()</code> returns cloned data to avoid holding references</li>
<li><strong>Shareable Storage</strong>: TensorStore clones share the same underlying data via
Arc</li>
</ul>
<h2 id="bloomfilter"><a class="header" href="#bloomfilter">BloomFilter</a></h2>
<p>The BloomFilter provides O(1) probabilistic rejection of non-existent keys,
useful for sparse key spaces where most lookups are misses.</p>
<h3 id="mathematical-foundation"><a class="header" href="#mathematical-foundation">Mathematical Foundation</a></h3>
<p>The Bloom filter uses optimal parameters calculated as:</p>
<p><strong>Bit array size</strong>: <code>m = -n * ln(p) / (ln(2)^2)</code></p>
<ul>
<li>Where n = expected items, p = false positive rate</li>
</ul>
<p><strong>Number of hash functions</strong>: <code>k = (m/n) * ln(2)</code></p>
<ul>
<li>Clamped to range [1, 16]</li>
</ul>
<h3 id="implementation-details"><a class="header" href="#implementation-details">Implementation Details</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct BloomFilter {
    bits: Box&lt;[AtomicU64]&gt;,  // Atomic u64 blocks for lock-free access
    num_bits: usize,
    num_hashes: usize,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Hash Function</strong>: Uses SipHash with different seeds for each hash function:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn hash_index&lt;K: Hash&gt;(&amp;self, key: &amp;K, seed: usize) -&gt; usize {
    let mut hasher = SipHasher::new_with_seed(seed as u64);
    key.hash(&amp;mut hasher);
    (hasher.finish() as usize) % self.num_bits
}
<span class="boring">}</span></code></pre></pre>
<h3 id="parameter-tuning-guide"><a class="header" href="#parameter-tuning-guide">Parameter Tuning Guide</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Expected Items</th><th>FP Rate</th><th>Bits</th><th>Hash Functions</th><th>Memory</th></tr></thead><tbody>
<tr><td>10,000</td><td>1%</td><td>95,851</td><td>7</td><td>~12 KB</td></tr>
<tr><td>10,000</td><td>0.1%</td><td>143,776</td><td>10</td><td>~18 KB</td></tr>
<tr><td>100,000</td><td>1%</td><td>958,506</td><td>7</td><td>~117 KB</td></tr>
<tr><td>1,000,000</td><td>1%</td><td>9,585,059</td><td>7</td><td>~1.2 MB</td></tr>
</tbody></table>
</div>
<p><strong>Gotchas</strong>:</p>
<ul>
<li>Bloom filter state is <strong>not persisted</strong> in snapshots; rebuild after load</li>
<li>Thread-safe via AtomicU64 with Relaxed ordering (eventual consistency)</li>
<li>Cannot remove items (use counting bloom filter for that case)</li>
<li>False positive rate increases if more items than expected are inserted</li>
</ul>
<h2 id="hnsw-index"><a class="header" href="#hnsw-index">HNSW Index</a></h2>
<p>Hierarchical Navigable Small World index for approximate nearest neighbor search
with O(log n) complexity.</p>
<h3 id="algorithm-overview"><a class="header" href="#algorithm-overview">Algorithm Overview</a></h3>
<pre class="mermaid">flowchart TD
    subgraph &quot;HNSW Structure&quot;
        L3[Layer 3: Entry Point] --&gt; L2[Layer 2: Skip connections]
        L2 --&gt; L1[Layer 1: More connections]
        L1 --&gt; L0[Layer 0: All nodes, dense connections]
    end

    subgraph &quot;Search Algorithm&quot;
        S1[Start at entry point, top layer] --&gt; S2[Greedy descent to layer 1]
        S2 --&gt; S3[At layer 0: ef-search candidates]
        S3 --&gt; S4[Return top-k results]
    end
</pre>
<h3 id="layer-selection"><a class="header" href="#layer-selection">Layer Selection</a></h3>
<p>New nodes are assigned layers using exponential distribution:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn random_level(&amp;self) -&gt; usize {
    let f = random_float_0_1();
    let level = (-f.ln() * self.config.ml).floor() as usize;
    level.min(32)  // Cap at 32 layers
}
<span class="boring">}</span></code></pre></pre>
<p>Where <code>ml = 1 / ln(m)</code> and m = connections per layer.</p>
<h3 id="hnswconfig-parameters"><a class="header" href="#hnswconfig-parameters">HNSWConfig Parameters</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Parameter</th><th>Default</th><th>Description</th></tr></thead><tbody>
<tr><td><code>m</code></td><td>16</td><td>Max connections per node per layer</td></tr>
<tr><td><code>m0</code></td><td>32</td><td>Max connections at layer 0 (2*m)</td></tr>
<tr><td><code>ef_construction</code></td><td>200</td><td>Candidates during construction</td></tr>
<tr><td><code>ef_search</code></td><td>50</td><td>Candidates during search</td></tr>
<tr><td><code>ml</code></td><td>1/ln(m)</td><td>Level multiplier</td></tr>
<tr><td><code>sparsity_threshold</code></td><td>0.5</td><td>Auto-sparse storage threshold</td></tr>
<tr><td><code>max_nodes</code></td><td>10,000,000</td><td>Capacity limit (prevents memory exhaustion)</td></tr>
</tbody></table>
</div>
<h3 id="configuration-presets"><a class="header" href="#configuration-presets">Configuration Presets</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// High recall (slower, more accurate)
HNSWConfig::high_recall()  // m=32, m0=64, ef_construction=400, ef_search=200

// High speed (faster, lower recall)
HNSWConfig::high_speed()   // m=8, m0=16, ef_construction=100, ef_search=20

// Custom configuration
HNSWConfig {
    m: 24,
    m0: 48,
    ef_construction: 300,
    ef_search: 100,
    ..Default::default()
}
<span class="boring">}</span></code></pre></pre>
<h3 id="simd-accelerated-distance"><a class="header" href="#simd-accelerated-distance">SIMD-Accelerated Distance</a></h3>
<p>Dense vector operations use 8-wide SIMD (f32x8):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn dot_product(a: &amp;[f32], b: &amp;[f32]) -&gt; f32 {
    let chunks = a.len() / 8;
    let mut sum = f32x8::ZERO;

    for i in 0..chunks {
        let offset = i * 8;
        let va = f32x8::from(&amp;a[offset..offset + 8]);
        let vb = f32x8::from(&amp;b[offset..offset + 8]);
        sum += va * vb;
    }

    // Sum lanes and handle remainder
    let arr: [f32; 8] = sum.into();
    let mut result: f32 = arr.iter().sum();
    // ... scalar remainder handling
}
<span class="boring">}</span></code></pre></pre>
<h3 id="neighbor-compression"><a class="header" href="#neighbor-compression">Neighbor Compression</a></h3>
<p>HNSW neighbor lists use delta-varint encoding for 3-8x compression:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct CompressedNeighbors {
    compressed: Vec&lt;u8&gt;,  // Delta-varint encoded neighbor IDs
}

// Decompression: O(n) where n = neighbor count
fn get(&amp;self) -&gt; Vec&lt;usize&gt; {
    decompress_ids(&amp;self.compressed)
}

// Compression: Sort + delta encode
fn set(&amp;mut self, ids: &amp;[usize]) {
    let mut sorted = ids.to_vec();
    sorted.sort_unstable();
    self.compressed = compress_ids(&amp;sorted);
}
<span class="boring">}</span></code></pre></pre>
<h3 id="storage-types"><a class="header" href="#storage-types">Storage Types</a></h3>
<pre class="mermaid">flowchart LR
    subgraph &quot;EmbeddingStorage&quot;
        D[Dense: Vec f32]
        S[Sparse: SparseVector]
        DV[Delta: DeltaVector]
        TT[TensorTrain: TTVectorCached]
    end

    D --&gt; |&quot;sparsity &gt; 50%&quot;| S
    D --&gt; |&quot;clusters around archetype&quot;| DV
    D --&gt; |&quot;high-dim 768+&quot;| TT
</pre>
<div class="table-wrapper"><table><thead><tr><th>Storage Type</th><th>Memory</th><th>Use Case</th><th>Distance Computation</th></tr></thead><tbody>
<tr><td>Dense</td><td>4 bytes/dim</td><td>General purpose</td><td>SIMD dot product</td></tr>
<tr><td>Sparse</td><td>6 bytes/nnz</td><td>&gt;50% zeros</td><td>Sparse-sparse O(nnz)</td></tr>
<tr><td>Delta</td><td>6 bytes/diff</td><td>Clustered embeddings</td><td>Via archetype</td></tr>
<tr><td>TensorTrain</td><td>8-10x compression</td><td>768+ dimensions</td><td>Native TT or reconstruct</td></tr>
</tbody></table>
</div>
<h3 id="edge-cases-and-gotchas"><a class="header" href="#edge-cases-and-gotchas">Edge Cases and Gotchas</a></h3>
<ol>
<li>
<p><strong>Delta vectors cannot be inserted directly</strong> - they require archetype
registry for distance computation. Convert to Dense first.</p>
</li>
<li>
<p><strong>TensorTrain storage</strong> - While stored in TT format, HNSW reconstructs to
dense for fast distance computation during search (native TT distance is
O(r^4) per comparison).</p>
</li>
<li>
<p><strong>Capacity limits</strong> - Default max_nodes=10M prevents memory exhaustion from
fuzzing/adversarial input. Use <code>try_insert</code> for graceful handling.</p>
</li>
<li>
<p><strong>Empty index</strong> - Entry point is <code>usize::MAX</code> when empty; search returns
empty results.</p>
</li>
</ol>
<h2 id="sparsevector"><a class="header" href="#sparsevector">SparseVector</a></h2>
<p>Memory-efficient storage for vectors with many zeros, based on the philosophy
that “zero represents absence of information, not a stored value.”</p>
<h3 id="internal-structure"><a class="header" href="#internal-structure">Internal Structure</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct SparseVector {
    dimension: usize,      // Total dimension (shell/boundary)
    positions: Vec&lt;u32&gt;,   // Sorted positions of non-zero values
    values: Vec&lt;f32&gt;,      // Corresponding values
}
<span class="boring">}</span></code></pre></pre>
<h3 id="operation-complexity-1"><a class="header" href="#operation-complexity-1">Operation Complexity</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Complexity</th><th>Notes</th></tr></thead><tbody>
<tr><td><code>from_dense</code></td><td>O(n)</td><td>Filters zeros</td></tr>
<tr><td><code>to_dense</code></td><td>O(n)</td><td>Reconstructs full vector</td></tr>
<tr><td><code>get(index)</code></td><td>O(log nnz)</td><td>Binary search</td></tr>
<tr><td><code>set(index, value)</code></td><td>O(nnz)</td><td>Insert/remove maintains sort</td></tr>
<tr><td><code>dot(sparse)</code></td><td>O(min(nnz_a, nnz_b))</td><td>Merge-join on positions</td></tr>
<tr><td><code>dot_dense(dense)</code></td><td>O(nnz)</td><td>Only access stored positions</td></tr>
<tr><td><code>add(sparse)</code></td><td>O(nnz_a + nnz_b)</td><td>Merge-based</td></tr>
<tr><td><code>cosine_similarity</code></td><td>O(nnz)</td><td>Using cached magnitudes</td></tr>
</tbody></table>
</div>
<h3 id="sparse-arithmetic-operations"><a class="header" href="#sparse-arithmetic-operations">Sparse Arithmetic Operations</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Create delta from before/after states (only stores differences)
let delta = SparseVector::from_diff(&amp;before, &amp;after, threshold);

// Subtraction: self - other
let diff = a.sub(&amp;b);

// Weighted average: (w1 * a + w2 * b) / (w1 + w2)
let merged = a.weighted_average(&amp;b, 0.7, 0.3);

// Project out conflicting component
let orthogonal = v.project_orthogonal(&amp;conflict_direction);
<span class="boring">}</span></code></pre></pre>
<h3 id="distance-metrics"><a class="header" href="#distance-metrics">Distance Metrics</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Metric</th><th>Range</th><th>Use Case</th></tr></thead><tbody>
<tr><td><code>cosine_similarity</code></td><td>[-1, 1]</td><td>Directional similarity</td></tr>
<tr><td><code>angular_distance</code></td><td>[0, PI]</td><td>Linear for small angles</td></tr>
<tr><td><code>geodesic_distance</code></td><td>[0, PI]</td><td>Arc length on unit sphere</td></tr>
<tr><td><code>jaccard_index</code></td><td>[0, 1]</td><td>Structural overlap (positions)</td></tr>
<tr><td><code>overlap_coefficient</code></td><td>[0, 1]</td><td>Subset containment</td></tr>
<tr><td><code>weighted_jaccard</code></td><td>[0, 1]</td><td>Value-weighted structural overlap</td></tr>
<tr><td><code>euclidean_distance</code></td><td>[0, inf)</td><td>L2 norm of difference</td></tr>
<tr><td><code>manhattan_distance</code></td><td>[0, inf)</td><td>L1 norm of difference</td></tr>
</tbody></table>
</div>
<h3 id="security-naninf-sanitization"><a class="header" href="#security-naninf-sanitization">Security: NaN/Inf Sanitization</a></h3>
<p>All similarity metrics sanitize results to prevent consensus ordering issues:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn cosine_similarity(&amp;self, other: &amp;SparseVector) -&gt; f32 {
    // ... computation ...

    // SECURITY: Sanitize result to valid range
    if result.is_nan() || result.is_infinite() {
        0.0
    } else {
        result.clamp(-1.0, 1.0)
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="memory-efficiency"><a class="header" href="#memory-efficiency">Memory Efficiency</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let sparse = SparseVector::from_dense(&amp;dense_vec);

// Metrics
sparse.sparsity()           // Fraction of zeros (0.0 - 1.0)
sparse.memory_bytes()       // Actual memory used
sparse.dense_memory_bytes() // Memory if stored dense
sparse.compression_ratio()  // Dense / Sparse ratio
<span class="boring">}</span></code></pre></pre>
<p>For a 1000-dim vector with 90% zeros:</p>
<ul>
<li>Dense: 4000 bytes</li>
<li>Sparse: ~800 bytes (100 positions <em>4 bytes + 100 values</em> 4 bytes)</li>
<li>Compression ratio: 5x</li>
</ul>
<h2 id="delta-vectors-and-archetype-registry"><a class="header" href="#delta-vectors-and-archetype-registry">Delta Vectors and Archetype Registry</a></h2>
<p>Delta encoding stores vectors as differences from reference “archetype” vectors,
providing significant compression for clustered embeddings.</p>
<h3 id="concept"><a class="header" href="#concept">Concept</a></h3>
<pre class="mermaid">flowchart LR
    subgraph &quot;Delta Encoding&quot;
        A[Archetype Vector] --&gt; |&quot;+ Delta&quot;| R[Reconstructed Vector]
        D[Delta: positions + values] --&gt; R
    end
</pre>
<p>When many embeddings cluster around common patterns:</p>
<ul>
<li>Identify archetype vectors (cluster centroids via k-means)</li>
<li>Store each embedding as: <code>archetype_id + sparse_delta</code></li>
<li>Reconstruct on demand: <code>archetype + delta = original</code></li>
</ul>
<h3 id="deltavector-structure"><a class="header" href="#deltavector-structure">DeltaVector Structure</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct DeltaVector {
    archetype_id: usize,       // Reference archetype
    dimension: usize,          // For reconstruction
    positions: Vec&lt;u16&gt;,       // Diff positions (u16 for memory)
    deltas: Vec&lt;f32&gt;,          // Delta values
    cached_magnitude: Option&lt;f32&gt;,  // For fast cosine similarity
}
<span class="boring">}</span></code></pre></pre>
<h3 id="optimized-dot-products"><a class="header" href="#optimized-dot-products">Optimized Dot Products</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// With precomputed archetype dot query
// Total: O(nnz) instead of O(dimension)
let result = delta.dot_dense_with_precomputed(query, archetype_dot_query);

// Between two deltas from SAME archetype
// dot(A, B) = dot(R, R) + dot(R, delta_b) + dot(delta_a, R) + dot(delta_a, delta_b)
let result = a.dot_same_archetype(&amp;b, archetype, archetype_magnitude_sq);
<span class="boring">}</span></code></pre></pre>
<h3 id="archetyperegistry"><a class="header" href="#archetyperegistry">ArchetypeRegistry</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Create registry with max 16 archetypes
let mut registry = ArchetypeRegistry::new(16);

// Discover archetypes via k-means clustering
let config = KMeansConfig {
    max_iterations: 100,
    convergence_threshold: 1e-4,
    seed: 42,
    init_method: KMeansInit::KMeansPlusPlus,  // Better but slower
};
registry.discover_archetypes(&amp;embeddings, 5, config);

// Encode vectors as deltas
let delta = registry.encode(&amp;vector, threshold)?;

// Analyze coverage
let stats = registry.analyze_coverage(&amp;vectors, 0.01);
// stats.avg_similarity, stats.avg_compression_ratio, stats.archetype_usage
<span class="boring">}</span></code></pre></pre>
<h3 id="persistence"><a class="header" href="#persistence">Persistence</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Save to TensorStore
registry.save_to_store(&amp;store)?;

// Load from TensorStore
let registry = ArchetypeRegistry::load_from_store(&amp;store, 16)?;
<span class="boring">}</span></code></pre></pre>
<h2 id="tiered-storage"><a class="header" href="#tiered-storage">Tiered Storage</a></h2>
<p>Two-tier storage with hot (in-memory) and cold (mmap) layers for
memory-efficient storage of large datasets.</p>
<h3 id="architecture"><a class="header" href="#architecture">Architecture</a></h3>
<pre class="mermaid">flowchart TD
    subgraph &quot;Hot Tier (In-Memory)&quot;
        H[MetadataSlab]
        I[ShardAccessTracker]
    end

    subgraph &quot;Cold Tier (Mmap)&quot;
        C[MmapStoreMut]
        CK[cold_keys HashSet]
    end

    GET --&gt; H
    H --&gt;|miss| CK
    CK --&gt;|found| C
    C --&gt;|promote| H

    PUT --&gt; H
    H --&gt;|migrate_cold| C
</pre>
<h3 id="tieredconfig"><a class="header" href="#tieredconfig">TieredConfig</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Default</th><th>Description</th></tr></thead><tbody>
<tr><td><code>cold_dir</code></td><td><code>PathBuf</code></td><td><code>/tmp/tensor_cold</code></td><td>Directory for cold storage files</td></tr>
<tr><td><code>cold_capacity</code></td><td><code>usize</code></td><td>64MB</td><td>Initial cold file size</td></tr>
<tr><td><code>sample_rate</code></td><td><code>u32</code></td><td>100</td><td>Access tracking sampling (100 = 1%)</td></tr>
</tbody></table>
</div>
<h3 id="migration-algorithm"><a class="header" href="#migration-algorithm">Migration Algorithm</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn migrate_cold(&amp;mut self, threshold_ms: u64) -&gt; Result&lt;usize&gt; {
    // 1. Find shards not accessed within threshold
    let cold_shards = self.instrumentation.cold_shards(threshold_ms);

    // 2. Collect keys belonging to cold shards
    let keys_to_migrate: Vec&lt;String&gt; = self.hot.scan("")
        .filter(|(key, _)| {
            let shard = shard_for_key(key);
            cold_shards.contains(&amp;shard)
        })
        .map(|(key, _)| key)
        .collect();

    // 3. Move to cold storage
    for key in keys_to_migrate {
        cold.insert(&amp;key, &amp;tensor)?;
        self.cold_keys.insert(key.clone());
        self.hot.delete(&amp;key);
    }

    cold.flush()?;
}
<span class="boring">}</span></code></pre></pre>
<h3 id="automatic-promotion"><a class="header" href="#automatic-promotion">Automatic Promotion</a></h3>
<p>When cold data is accessed, it’s automatically promoted back to hot:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn get(&amp;mut self, key: &amp;str) -&gt; Result&lt;TensorData&gt; {
    // Try hot first
    if let Some(data) = self.hot.get(key) {
        return Ok(data);
    }

    // Try cold
    if self.cold_keys.contains(key) {
        let tensor = self.cold.get(key)?;

        // Promote to hot
        self.hot.set(key, tensor.clone());
        self.cold_keys.remove(key);
        self.migrations_to_hot.fetch_add(1, Ordering::Relaxed);

        return Ok(tensor);
    }

    Err(TensorStoreError::NotFound(key))
}
<span class="boring">}</span></code></pre></pre>
<h3 id="statistics"><a class="header" href="#statistics">Statistics</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let stats = store.stats();
// stats.hot_count, stats.cold_count
// stats.hot_lookups, stats.cold_lookups, stats.cold_hits
// stats.migrations_to_cold, stats.migrations_to_hot
<span class="boring">}</span></code></pre></pre>
<h2 id="access-instrumentation"><a class="header" href="#access-instrumentation">Access Instrumentation</a></h2>
<p>Low-overhead tracking of shard access patterns for intelligent memory tiering.</p>
<h3 id="shardaccesstracker"><a class="header" href="#shardaccesstracker">ShardAccessTracker</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct ShardAccessTracker {
    shards: Box&lt;[ShardStats]&gt;,     // Per-shard counters
    shard_count: usize,            // Default: 16
    start_time: Instant,           // For last_access timestamps
    sample_rate: u32,              // 1 = every access, 100 = 1%
    sample_counter: AtomicU64,     // For sampling
}

// Sampling logic
fn should_sample(&amp;self) -&gt; bool {
    if self.sample_rate == 1 { return true; }
    self.sample_counter.fetch_add(1, Relaxed).is_multiple_of(self.sample_rate)
}
<span class="boring">}</span></code></pre></pre>
<h3 id="hotcold-detection"><a class="header" href="#hotcold-detection">Hot/Cold Detection</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Get shards sorted by access count (hottest first)
let hot = tracker.hot_shards(5);  // Top 5 hottest

// Get shards not accessed within threshold
let cold = tracker.cold_shards(30_000);  // Not accessed in 30s
<span class="boring">}</span></code></pre></pre>
<h3 id="hnsw-access-stats"><a class="header" href="#hnsw-access-stats">HNSW Access Stats</a></h3>
<p>Specialized instrumentation for HNSW index:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct HNSWAccessStats {
    entry_point_accesses: AtomicU64,
    layer0_traversals: AtomicU64,
    upper_layer_traversals: AtomicU64,
    total_searches: AtomicU64,
    distance_calculations: AtomicU64,
}

// Snapshot metrics
let stats = hnsw.access_stats()?;
stats.layer0_ratio()          // Layer 0 work fraction
stats.avg_distances_per_search  // Distance calcs per search
stats.searches_per_second()   // Throughput
<span class="boring">}</span></code></pre></pre>
<h2 id="configuration-options"><a class="header" href="#configuration-options">Configuration Options</a></h2>
<h3 id="slabrouterconfig"><a class="header" href="#slabrouterconfig">SlabRouterConfig</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Default</th><th>Description</th></tr></thead><tbody>
<tr><td><code>embedding_dim</code></td><td><code>usize</code></td><td>384</td><td>Embedding dimension for EmbeddingSlab</td></tr>
<tr><td><code>cache_capacity</code></td><td><code>usize</code></td><td>10,000</td><td>Cache capacity for CacheRing</td></tr>
<tr><td><code>cache_strategy</code></td><td><code>EvictionStrategy</code></td><td>Default</td><td>Eviction strategy (LRU/LFU)</td></tr>
<tr><td><code>blob_segment_size</code></td><td><code>usize</code></td><td>64MB</td><td>Segment size for BlobLog</td></tr>
<tr><td><code>graph_merge_threshold</code></td><td><code>usize</code></td><td>10,000</td><td>Merge threshold for GraphTensor</td></tr>
</tbody></table>
</div>
<h2 id="usage-examples"><a class="header" href="#usage-examples">Usage Examples</a></h2>
<h3 id="basic-operations"><a class="header" href="#basic-operations">Basic Operations</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let store = TensorStore::new();

// Store a tensor
let mut user = TensorData::new();
user.set("name", TensorValue::Scalar(ScalarValue::String("Alice".into())));
user.set("age", TensorValue::Scalar(ScalarValue::Int(30)));
user.set("embedding", TensorValue::Vector(vec![0.1, 0.2, 0.3, 0.4]));
store.put("user:1", user)?;

// Retrieve
let data = store.get("user:1")?;

// Scan by prefix
let user_keys = store.scan("user:");
let count = store.scan_count("user:");
<span class="boring">}</span></code></pre></pre>
<h3 id="with-bloom-filter"><a class="header" href="#with-bloom-filter">With Bloom Filter</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Fast rejection of non-existent keys
let store = TensorStore::with_bloom_filter(10_000, 0.01);
store.put("key:1", tensor)?;

// O(1) rejection if key definitely doesn't exist
if store.exists("key:999") { /* ... */ }
<span class="boring">}</span></code></pre></pre>
<h3 id="with-instrumentation"><a class="header" href="#with-instrumentation">With Instrumentation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Enable access tracking with 1% sampling
let store = TensorStore::with_instrumentation(100);

// After operations, check access patterns
let snapshot = store.access_snapshot()?;
println!("Hot shards: {:?}", store.hot_shards(5)?);
println!("Cold shards: {:?}", store.cold_shards(30_000)?);
<span class="boring">}</span></code></pre></pre>
<h3 id="shared-storage-across-engines"><a class="header" href="#shared-storage-across-engines">Shared Storage Across Engines</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let store = TensorStore::new();

// Clone shares the same underlying Arc&lt;SlabRouter&gt;
let store_clone = store.clone();

// Both see the same data
store.put("user:1", user_data)?;
assert!(store_clone.exists("user:1"));

// Use with multiple engines
let vector_engine = VectorEngine::with_store(store.clone());
let graph_engine = GraphEngine::with_store(store.clone());
<span class="boring">}</span></code></pre></pre>
<h3 id="persistence-1"><a class="header" href="#persistence-1">Persistence</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Save snapshot
store.save_snapshot("data.bin")?;

// Load snapshot
let store = TensorStore::load_snapshot("data.bin")?;

// Load with Bloom filter rebuild
let store = TensorStore::load_snapshot_with_bloom_filter(
    "data.bin",
    10_000,   // expected items
    0.01      // false positive rate
)?;

// Compressed snapshot
use tensor_compress::{CompressionConfig, QuantMode};
let config = CompressionConfig {
    vector_quantization: Some(QuantMode::Int8),  // 4x compression
    delta_encoding: true,
    rle_encoding: true,
};
store.save_snapshot_compressed("data.bin", config)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="tiered-storage-1"><a class="header" href="#tiered-storage-1">Tiered Storage</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tensor_store::{TieredStore, TieredConfig};

let config = TieredConfig {
    cold_dir: "/data/cold".into(),
    cold_capacity: 64 * 1024 * 1024,
    sample_rate: 100,
};

let mut store = TieredStore::new(config)?;
store.put("user:1", tensor);

// Migrate cold data (not accessed in 30s)
let migrated = store.migrate_cold(30_000)?;

// Check stats
let stats = store.stats();
println!("Hot: {}, Cold: {}", stats.hot_count, stats.cold_count);
<span class="boring">}</span></code></pre></pre>
<h3 id="hnsw-index-1"><a class="header" href="#hnsw-index-1">HNSW Index</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let index = HNSWIndex::with_config(HNSWConfig::default());

// Insert dense, sparse, or auto-select
index.insert(vec![0.1, 0.2, 0.3]);
index.insert_sparse(sparse_vec);
index.insert_auto(mixed_vec);  // Auto-selects dense/sparse

// With capacity checking
match index.try_insert(vec) {
    Ok(id) =&gt; println!("Inserted as node {}", id),
    Err(EmbeddingStorageError::CapacityExceeded { limit, current }) =&gt; {
        println!("Index full: {} / {}", current, limit);
    }
}

// Search with custom ef
let results = index.search_with_ef(&amp;query, 10, 100);
for (id, similarity) in results {
    println!("Node {}: {:.4}", id, similarity);
}
<span class="boring">}</span></code></pre></pre>
<h3 id="delta-encoded-embeddings"><a class="header" href="#delta-encoded-embeddings">Delta-Encoded Embeddings</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut registry = ArchetypeRegistry::new(16);

// Discover archetypes from existing embeddings
registry.discover_archetypes(&amp;embeddings, 5, KMeansConfig::default());

// Encode new vectors as deltas
let results = registry.encode_batch(&amp;embeddings, 0.01);
for (delta, compression_ratio) in results {
    println!("Archetype {}, compression: {:.2}x",
             delta.archetype_id(), compression_ratio);
}
<span class="boring">}</span></code></pre></pre>
<h2 id="error-types"><a class="header" href="#error-types">Error Types</a></h2>
<h3 id="tensorstoreerror"><a class="header" href="#tensorstoreerror">TensorStoreError</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Error</th><th>Cause</th></tr></thead><tbody>
<tr><td><code>NotFound(key)</code></td><td><code>get</code> or <code>delete</code> on nonexistent key</td></tr>
</tbody></table>
</div>
<h3 id="snapshoterror"><a class="header" href="#snapshoterror">SnapshotError</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Error</th><th>Cause</th></tr></thead><tbody>
<tr><td><code>IoError(std::io::Error)</code></td><td>File not found, permission denied, disk full</td></tr>
<tr><td><code>SerializationError(String)</code></td><td>Corrupted file, incompatible format</td></tr>
</tbody></table>
</div>
<h3 id="tierederror"><a class="header" href="#tierederror">TieredError</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Error</th><th>Cause</th></tr></thead><tbody>
<tr><td><code>Store(TensorStoreError)</code></td><td>Underlying store error</td></tr>
<tr><td><code>Mmap(MmapError)</code></td><td>Memory-mapped file error</td></tr>
<tr><td><code>Io(std::io::Error)</code></td><td>I/O error</td></tr>
<tr><td><code>NotConfigured</code></td><td>Cold storage not configured</td></tr>
</tbody></table>
</div>
<h3 id="embeddingstorageerror"><a class="header" href="#embeddingstorageerror">EmbeddingStorageError</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Error</th><th>Cause</th></tr></thead><tbody>
<tr><td><code>DeltaRequiresRegistry</code></td><td>Delta storage used without archetype registry</td></tr>
<tr><td><code>ArchetypeNotFound(id)</code></td><td>Referenced archetype not in registry</td></tr>
<tr><td><code>CapacityExceeded { limit, current }</code></td><td>HNSW index at max_nodes limit</td></tr>
<tr><td><code>DeltaNotSupported</code></td><td>Delta vectors inserted into HNSW (unsupported)</td></tr>
</tbody></table>
</div>
<h2 id="related-modules"><a class="header" href="#related-modules">Related Modules</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Module</th><th>Relationship</th></tr></thead><tbody>
<tr><td><code>relational_engine</code></td><td>Uses TensorStore for table row storage</td></tr>
<tr><td><code>graph_engine</code></td><td>Uses TensorStore for node/edge storage</td></tr>
<tr><td><code>vector_engine</code></td><td>Uses TensorStore + HNSWIndex for embeddings</td></tr>
<tr><td><code>tensor_compress</code></td><td>Provides compression for snapshots</td></tr>
<tr><td><code>tensor_checkpoint</code></td><td>Uses TensorStore snapshots for atomic restore</td></tr>
<tr><td><code>tensor_chain</code></td><td>Uses TensorStore for blockchain state</td></tr>
</tbody></table>
</div>
<h2 id="dependencies"><a class="header" href="#dependencies">Dependencies</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Crate</th><th>Purpose</th></tr></thead><tbody>
<tr><td><code>serde</code></td><td>Serialization</td></tr>
<tr><td><code>bincode</code></td><td>Binary snapshot format</td></tr>
<tr><td><code>tensor_compress</code></td><td>Compression algorithms</td></tr>
<tr><td><code>wide</code></td><td>SIMD operations (f32x8)</td></tr>
<tr><td><code>memmap2</code></td><td>Memory-mapped files</td></tr>
<tr><td><code>fxhash</code></td><td>Fast hashing</td></tr>
<tr><td><code>parking_lot</code></td><td>Efficient locks</td></tr>
<tr><td><code>bitvec</code></td><td>Bit vectors for bloom filter</td></tr>
</tbody></table>
</div>
                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../architecture/overview.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../architecture/relational-engine.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../architecture/overview.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../architecture/relational-engine.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>



        <script>
            window.playground_line_numbers = true;
        </script>

        <script>
            window.playground_copyable = true;
        </script>

        <script src="../ace.js"></script>
        <script src="../mode-rust.js"></script>
        <script src="../editor.js"></script>
        <script src="../theme-dawn.js"></script>
        <script src="../theme-tomorrow_night.js"></script>

        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
